import sys
import os
import warnings
import traceback
import logging
import findspark
import time
from datetime import datetime
from colorama import Fore, Back, Style, init

init()

findspark.init("/opt/spark")
from elasticsearch import Elasticsearch, helpers
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import *

def print_banner():
    banner = f"""
{Fore.CYAN}
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘       SPARK STREAMING TO ELASTICSEARCH            â•‘
    â•‘                Version 3.1                        â•‘
    â•‘ ----------------------------------------         â•‘
    â•‘  ğŸ“¥ Kafka â†’ ğŸ”„ Spark â†’ ğŸ’¾ Elasticsearch â†’ ğŸ“Š Kibana â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{Style.RESET_ALL}"""
    print(banner)

def log_message(message, level="info"):
    timestamp = datetime.now().strftime("%H:%M:%S")
    colors = {
        "info": Fore.GREEN,
        "warning": Fore.YELLOW,
        "error": Fore.RED,
        "highlight": Fore.BLUE,
        "success": Fore.CYAN
    }
    print(f"{colors.get(level, Fore.WHITE)}[{timestamp}] {message}{Style.RESET_ALL}")

def print_batch_report(batch_id, count, duration, rate):
    report = f"""
{Fore.CYAN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• BATCH PERFORMANS RAPORU â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ†” Batch ID    : {batch_id:<37} â•‘
â•‘ ğŸ“Š Ä°ÅŸlem Ã–zeti:                                        â•‘
â•‘   â”œâ”€â”€ ğŸ“ Toplam KayÄ±t  : {count:<28} â•‘
â•‘   â””â”€â”€ â±ï¸  Ä°ÅŸlem SÃ¼resi : {duration:.2f} saniye{' ':<20} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Style.RESET_ALL}
"""
    print(report)

print_banner()
log_message("Sistem baÅŸlatÄ±lÄ±yor...", "highlight")
warnings.filterwarnings('ignore')
checkpointDir = "file:///tmp/streaming/kafka_office_input"

# Spark Session baÅŸlatma
try:
    log_message("ğŸš€ Spark Session oluÅŸturuluyor...")
    spark = (SparkSession.builder
             .appName("Streaming Kafka-Spark")
             .master("local[2]")
             .config("spark.driver.memory", "4g")
             .config("spark.executor.memory", "4g")
             .config("spark.sql.shuffle.partitions", "30")
             .config("spark.default.parallelism", "30")
             .config("spark.network.timeout", "800s")
             .config("spark.executor.heartbeatInterval", "60s")
             .config("spark.storage.blockManagerSlaveTimeoutMs", "800s")
             .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
             .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1")
             .config("spark.sql.legacy.timeParserPolicy", "LEGACY")
             .config("spark.memory.fraction", "0.6")
             .config("spark.memory.storageFraction", "0.3")
             .getOrCreate())

    log_message("âœ… Spark Session baÅŸarÄ±yla oluÅŸturuldu!", "success")
except Exception:
    traceback.print_exc(file=sys.stderr)
    log_message("âŒ Spark Session oluÅŸturma hatasÄ±!", "error")
    sys.exit(1)
# Kafka'dan veri okuma
try:
    log_message("ğŸ“¥ Kafka baÄŸlantÄ±sÄ± kuruluyor...")
    df = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "kafka:9092") \
        .option("subscribe", "office-input") \
        .option("startingOffsets", "latest") \
        .load()
    log_message("âœ… Kafka baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±!", "success")
except Exception as e:
    log_message(f"âŒ Kafka baÄŸlantÄ± hatasÄ±: {e}", "error")
    sys.exit(1)

# Veri dÃ¶nÃ¼ÅŸÃ¼mleri
log_message("ğŸ”„ Veri dÃ¶nÃ¼ÅŸÃ¼mleri baÅŸlatÄ±lÄ±yor...")

df2 = df.selectExpr("CAST(value AS STRING)")
df3 = df2.withColumn("timestamp", F.split(F.col("value"), ",")[0]) \
    .withColumn("ts_min_bignt", F.split(F.col("value"), ",")[1].cast(IntegerType())) \
    .withColumn("room", F.split(F.col("value"), ",")[2]) \
    .withColumn("co2", F.split(F.col("value"), ",")[3].cast(FloatType())) \
    .withColumn("light", F.split(F.col("value"), ",")[4].cast(FloatType())) \
    .withColumn("temp", F.split(F.col("value"), ",")[5].cast(FloatType())) \
    .withColumn("humidity", F.split(F.col("value"), ",")[6].cast(FloatType())) \
    .withColumn("pir", F.split(F.col("value"), ",")[7].cast(FloatType())) \
    .withColumn("event_ts_min", 
        F.when(F.to_timestamp(F.col("timestamp"), "yyyy-MM-dd HH:mm:ss").isNull(),
               F.current_timestamp().cast("long") * 1000)
        .otherwise(F.unix_timestamp(
            F.to_timestamp(F.col("timestamp"), "yyyy-MM-dd HH:mm:ss")
        ).cast("long") * 1000)) \
    .drop(F.col("value")) \
    .drop(F.col("timestamp"))

df3.createOrReplaceTempView("df3")
log_message("âœ… Veri dÃ¶nÃ¼ÅŸÃ¼mleri tamamlandÄ±", "success")

# Hareket durumu hesaplama
log_message("ğŸ”„ Hareket analizi yapÄ±lÄ±yor...")
df4 = spark.sql("""
   SELECT
       event_ts_min,
       co2,
       humidity,
       light,
       temp,
       room,
       pir,
       CASE
           WHEN pir > 0.0 THEN 'movement'
           ELSE 'no_movement'
       END as if_movement
   FROM df3
""")
# Elasticsearch baÄŸlantÄ±sÄ± ve index oluÅŸturma
try:
    log_message("ğŸ“¡ Elasticsearch baÄŸlantÄ±sÄ± kuruluyor...")
    es = Elasticsearch(
        ["http://es:9200"],
        verify_certs=False,
        timeout=30,
        retry_on_timeout=True,
        max_retries=3
    )
    
    # Index kontrolÃ¼ ve mapping
    if es.indices.exists(index="office_input"):
        es.indices.delete(index="office_input")
        log_message("ğŸ—‘ï¸ Eski index silindi", "warning")
        time.sleep(2)

    index_mapping = {
        "mappings": {
            "properties": {
                "event_ts_min": {"type": "date"},
                "co2": {"type": "float"},
                "humidity": {"type": "float"},
                "light": {"type": "float"},
                "temperature": {"type": "float"},
                "room": {"type": "keyword"},
                "pir": {"type": "float"},
                "if_movement": {"type": "keyword"}
            }
        },
        "settings": {
            "number_of_shards": 2,
            "number_of_replicas": 0,
            "refresh_interval": "1s"
        }
    }

    es.indices.create(index="office_input", body=index_mapping)
    log_message("âœ… Index baÅŸarÄ±yla oluÅŸturuldu!", "success")

except Exception as e:
    log_message(f"âŒ Elasticsearch iÅŸlem hatasÄ±: {e}", "error")
    sys.exit(1)

# Streaming iÅŸlemi
log_message("\nğŸš€ Streaming baÅŸlatÄ±lÄ±yor...")

def process_batch(batch_df, batch_id):
    batch_start_time = time.time()
    
    try:
        rows = batch_df.collect()
        bulk_data = []
        chunk_size = 500
        total_rows = len(rows)
        processed_rows = 0

        for row in rows:
            try:
                doc = {
                    "event_ts_min": row.event_ts_min,
                    "co2": float(row.co2) if row.co2 is not None else 0.0,
                    "humidity": float(row.humidity) if row.humidity is not None else 0.0,
                    "light": float(row.light) if row.light is not None else 0.0,
                    "temperature": float(row.temp) if row.temp is not None else 0.0,
                    "room": row.room,
                    "pir": float(row.pir) if row.pir is not None else 0.0,
                    "if_movement": row.if_movement
                }
                
                bulk_data.append({
                    '_index': 'office_input',
                    '_source': doc
                })

                processed_rows += 1
                if len(bulk_data) >= chunk_size:
                    success, failed = helpers.bulk(es, bulk_data, stats_only=True)
                    log_message(f"âœ… Chunk yazÄ±ldÄ± - BaÅŸarÄ±lÄ±: {success}, BaÅŸarÄ±sÄ±z: {failed}", "success")
                    bulk_data = []

            except Exception as row_error:
                log_message(f"âš ï¸ SatÄ±r dÃ¶nÃ¼ÅŸÃ¼m hatasÄ±: {str(row_error)}", "warning")
                continue

        if bulk_data:
            success, failed = helpers.bulk(es, bulk_data, stats_only=True)
            log_message(f"âœ… Son chunk yazÄ±ldÄ± - BaÅŸarÄ±lÄ±: {success}, BaÅŸarÄ±sÄ±z: {failed}", "success")

        batch_duration = time.time() - batch_start_time
        print_batch_report(batch_id, total_rows, batch_duration, total_rows/batch_duration)

    except Exception as e:
        log_message(f"âŒ Batch iÅŸleme hatasÄ±: {str(e)}", "error")
        raise e

try:
    query = (df4.writeStream
             .foreachBatch(process_batch)
             .option("checkpointLocation", checkpointDir)
             .trigger(processingTime='20 seconds')
             .start())

    log_message("""
    âœ¨ Stream baÅŸarÄ±yla baÅŸlatÄ±ldÄ±!
    ğŸ“Š Veri akÄ±ÅŸÄ± detaylarÄ±:
    â”œâ”€â”€ ğŸ“¥ Kaynak: Kafka (office-input topic)
    â”œâ”€â”€ ğŸ’¾ Hedef: Elasticsearch (office_input index)
    â””â”€â”€ ğŸ”„ Checkpoint: /tmp/streaming/kafka_office_input
    """, "success")

    query.awaitTermination()

except Exception as e:
    log_message(f"âŒ Stream baÅŸlatma hatasÄ±: {e}", "error")
    sys.exit(1)

log_message("âœ¨ Ä°ÅŸlem tamamlandÄ±!", "success")